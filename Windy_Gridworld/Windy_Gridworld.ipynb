{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Names\n",
    "## Aprendizaje de Máquina Avanzado\n",
    "### Proyecto Final Individual\n",
    "#### Fernanda Alcalá Durand\n",
    "\n",
    "En este proyecto, se programará un agente que aprenda a moverse dentro de una cuadrícula para encontrar la meta en el menor número de pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Mostrar código</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython.core.display as di\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Mostrar código</button>''', raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## IMPORTANT NOTE: THIS RUNS ON PYTHON 3\n",
    "\n",
    "# This is me trying to do a very baby version of this:\n",
    "# http://cs.stançford.edu/people/karpathy/reinforcejs/gridworld_td.html\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## NECESSARY PARAMETERS ##########\n",
    "#### World parameters\n",
    "world_width = 7\n",
    "world_length = 10\n",
    "start = (3,0)\n",
    "goal = (3,7)\n",
    "# don't know about this one\n",
    "goal_reward = 100\n",
    "#### Learning parameters\n",
    "gam = 0.9\n",
    "# don't know about this one\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## CREATE ENVIRONMENT ##########\n",
    "\n",
    "########### WIND\n",
    "wind_matrix = np.zeros((world_width,world_length))\n",
    "\n",
    "#values for each one of the columns\n",
    "wind_values = (0,0,0,1,1,1,2,1,-1,0)\n",
    "\n",
    "for i in range(len(wind_values)):\n",
    "    x = np.ones(world_width) * wind_values[i]\n",
    "    wind_matrix[:,i] = x\n",
    "\n",
    "##########\n",
    "#reward matrix\n",
    "r_matrix = np.zeros((world_width,world_length))\n",
    "r_matrix[goal] = goal_reward  #this is the only info we have at the beginning\n",
    "#movement matrix\n",
    "m_matrix = np.chararray((world_width,world_length))\n",
    "m_matrix[goal] = ('X')  #this is the only info we have at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo se ve el mundo al comenzar?\n",
    "\n",
    "Lo único que conocemos en este momento es que la posición que asignamos como \"goal\" tiene recompensa. El agente no sabe esto: lo único que sabe es que tiene que encontrar un policy que maximice su valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEACAYAAAB4ayemAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQog2ABRCtAGgkN8DYp7/deII6+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f436eb0f2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = plt.pcolor(r_matrix,cmap=matplotlib.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## DEFINING ACTORS AND ENVIRONMENT, AND HOW THEY INTERACT ##########\n",
    "\n",
    "# Divide the world in squares and define rules about how to move - this includes the wind\n",
    "\n",
    "class Square:\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.wind = int(wind_matrix[x,y])\n",
    "\n",
    "# Possible actions\n",
    "possible_actions = [ 'N', 'E', 'S', 'W' , 'X']   #X means don't move   \n",
    "              \n",
    "# Convert [ N, E, S, W ] to actual coordi      \n",
    "def convertAction(action):\n",
    "    if(action == 'N'):\n",
    "        return Square(0,1)\n",
    "    elif(action == 'E'):\n",
    "        return Square(1,0)\n",
    "    elif(action == 'S'):\n",
    "        return Square(0,-1)\n",
    "    elif(action == 'W'):\n",
    "        return Square(-1,0)\n",
    "    else:\n",
    "        return Square(0,0)\n",
    "\n",
    "#let's see how the agent moves on the grid    \n",
    "def moveAgent(from_sq,action):\n",
    "    \n",
    "    movement = convertAction(action) # action is a char, see convertAction\n",
    "\n",
    "    to_sq = Square(from_sq.x + movement.x , from_sq.y + movement.y)\n",
    "    \n",
    "    #if it tries to leave the grid, it stays on the square where it was\n",
    "    if( (to_sq.x < 0) or (to_sq.x + 1 > world_width) or (to_sq.y < 0) or (to_sq.y +1 > world_length) ):\n",
    "        moves_to = from_sq\n",
    "        \n",
    "    #wind can't take it out of the grid\n",
    "    elif( to_sq.y + to_sq.wind + 1 > world_width):\n",
    "        to_sq.y = world_width\n",
    "        moves_to = to_sq\n",
    "        \n",
    "    #else, we sum the value of wind to the y coordinate\n",
    "    else:\n",
    "        to_sq.y = to_sq.y + to_sq.wind\n",
    "        moves_to = to_sq\n",
    "        \n",
    "    return moves_to\n",
    "\n",
    "#get max reward for all possible actions\n",
    "def maxReward(state):\n",
    "    all_rewards = np.zeros(len(possible_actions))\n",
    "    rewards_for_index = []\n",
    "    \n",
    "    for i in range(len(possible_actions)):\n",
    "        moved_to = moveAgent( state, possible_actions[i] )\n",
    "        reward = r_matrix[moved_to.x,moved_to.y]\n",
    "        all_rewards[i] = reward\n",
    "        rewards_for_index.append(reward)\n",
    "        \n",
    "    max_reward = max(all_rewards)\n",
    "    \n",
    "    best_action_index = rewards_for_index.index(max_reward)\n",
    "    best_action = possible_actions[best_action_index]\n",
    "    \n",
    "    return max_reward, best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q- Learning\n",
    "\n",
    "Hagamos que el agente explore y aprenda qué acción tiene que tomar en cada una de las casillas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How rewards are updated\n",
    "# Q(state, action) = R(state, action) + Gamma * max[Q(next state, all actions)]\n",
    "def Q(state,action):\n",
    "    \n",
    "    if(state == goal):   #if he's at the goal, Q value doesn't update and he doesn't look for actions afterwards\n",
    "        Q_value = goal_reward\n",
    "        best_action = (0,0)\n",
    "    \n",
    "    else:\n",
    "        state_Square = Square( state[0] , state[1] )\n",
    "    \n",
    "        max_reward,best_action = maxReward(state_Square)\n",
    "    \n",
    "        if r_matrix[state] == 0:\n",
    "            Q_value = r_matrix[state] + gam * max_reward\n",
    "        else:\n",
    "            Q_value = r_matrix[state]\n",
    "        \n",
    "        r_matrix[state] = Q_value  #reward matrix is updated\n",
    "        m_matrix[state] = best_action #same for movement matrix\n",
    "    \n",
    "    return Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in range(max_iter):\n",
    "    for i in range(6):\n",
    "        for j in range(9):\n",
    "            Q( (i,j), 'N' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEACAYAAAB4ayemAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEG0ASIRoA0AiRBsAEiHaAJAI0QaARP4L4/0izn/tKMkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f436c6905f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = plt.pcolor(r_matrix,cmap=matplotlib.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de movimientos indicados sería la siguiente. Recordemos que cada entrada nos muestra la mejor acción a tomar, suponiendo que el agente se encuentra en esa casilla. X marks the spot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'N' b'N' b'N' b'N' b'E' b'E' b'E' b'E' b'E' b'\\xed']\n",
      " [b'N' b'N' b'N' b'N' b'N' b'E' b'E' b'E' b'E' b'\\x8c']\n",
      " [b'N' b'N' b'N' b'N' b'N' b'N' b'E' b'E' b'E' b'\\x7f']\n",
      " [b'N' b'N' b'N' b'N' b'N' b'N' b'N' b'X' b'N' '']\n",
      " [b'N' b'N' b'N' b'N' b'N' b'N' b'W' b'W' b'W' b'-']\n",
      " [b'N' b'N' b'N' b'N' b'N' b'W' b'W' b'W' b'W' b'\\x95']\n",
      " [b'C' b'\\x7f' '' '' b'P' '' '' '' '' '']]\n"
     ]
    }
   ],
   "source": [
    "print(m_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How policy is updated\n",
    "def Pi(start,goal):\n",
    "    \n",
    "    state_Square = Square( start[0] , start[1] )\n",
    "    goal_Square = Square( goal[0] , goal[1] )\n",
    "    tries = 0\n",
    "    policy = []\n",
    "    discounted_rewards = []\n",
    "    \n",
    "    print(\"El agente empieza en (\", state_Square.x,\",\",state_Square.y, \")\")\n",
    "    \n",
    "    while(state_Square != goal_Square and (tries < 20)):\n",
    "\n",
    "        acciones = ['N','S','E','W']\n",
    "        \n",
    "        try:\n",
    "            derecha = r_matrix[state_Square.x,state_Square.y+1]\n",
    "        except: \n",
    "          pass \n",
    "        try:\n",
    "            izquierda = r_matrix[state_Square.x,state_Square.y-1]\n",
    "        except: \n",
    "          pass\n",
    "        try:\n",
    "            arriba = r_matrix[state_Square.x-1,state_Square.y]\n",
    "        except: \n",
    "          pass \n",
    "        try:\n",
    "            abajo = r_matrix[state_Square.x+1,state_Square.y]\n",
    "        except: \n",
    "          pass\n",
    "           \n",
    "        rewards = (arriba,abajo,derecha+1,izquierda)\n",
    "        \n",
    "        biggest_reward = max(rewards)\n",
    "        best_action = acciones[rewards.index(biggest_reward)]\n",
    "    \n",
    "        discounted_rewards.append(biggest_reward)\n",
    "        policy.append(best_action)\n",
    "        \n",
    "        #exploracion contra explotacion\n",
    "        p = np.random.uniform(0,1,1)\n",
    "        if p > 0.5:\n",
    "            try:\n",
    "                state_Square = moveAgent(state_Square,best_action)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                state_Square = moveAgent(state_Square,random.choice(acciones))\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "\n",
    "        print(\"El agente se mueve a (\", state_Square.x,\",\",state_Square.y, \")\")\n",
    "        \n",
    "        tries = tries +1\n",
    "\n",
    "    print(\"El agente obtiene la recompensa (\", discounted_rewards[tries-1] ,\")\")\n",
    "    \n",
    "    return discounted_rewards, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy óptima que pudo encontrar el agente fue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El agente empieza en ( 3 , 0 )\n",
      "El agente se mueve a ( 4 , 0 )\n",
      "El agente se mueve a ( 5 , 0 )\n",
      "El agente se mueve a ( 6 , 0 )\n",
      "El agente se mueve a ( 6 , 0 )\n",
      "El agente se mueve a ( 6 , 0 )\n",
      "El agente se mueve a ( 6 , 1 )\n",
      "El agente se mueve a ( 6 , 1 )\n",
      "El agente se mueve a ( 6 , 2 )\n",
      "El agente se mueve a ( 6 , 4 )\n",
      "El agente se mueve a ( 6 , 6 )\n",
      "El agente se mueve a ( 6 , 7 )\n",
      "El agente se mueve a ( 6 , 7 )\n",
      "El agente se mueve a ( 5 , 7 )\n",
      "El agente se mueve a ( 4 , 7 )\n",
      "El agente se mueve a ( 5 , 7 )\n",
      "El agente se mueve a ( 4 , 7 )\n",
      "El agente se mueve a ( 4 , 7 )\n",
      "El agente se mueve a ( 4 , 7 )\n",
      "El agente se mueve a ( 4 , 7 )\n",
      "El agente se mueve a ( 4 , 7 )\n",
      "El agente obtiene la recompensa ( 100.0 )\n"
     ]
    }
   ],
   "source": [
    "rewards,policy = Pi(start,goal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
